{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.linalg import cholesky, inv, solve\n",
    "from scipy.stats import wishart, invwishart\n",
    "from lifetimes import BetaGeoFitter, GammaGammaFitter\n",
    "from lifetimes.utils import calibration_and_holdout_data, summary_data_from_transaction_data\n",
    "from lifetimes.plotting import plot_calibration_purchases_vs_holdout_purchases, plot_period_transactions\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(datafile, parse_dates=None):\n",
    "    df = pd.read_csv(datafile, delimiter=',', parse_dates=parse_dates)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_datafolder = '/development/data'\n",
    "g_customer_dataset = '{}/olist_customers_dataset.csv'.format(g_datafolder)\n",
    "g_orders_dataset = '{}/olist_orders_dataset.csv'.format(g_datafolder)\n",
    "g_payments_dataset = '{}/olist_order_payments_dataset.csv'.format(g_datafolder)\n",
    "g_orderitems_dataset = '{}/olist_order_items_dataset.csv'.format(g_datafolder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "customer_df = load_dataset(g_customer_dataset)\n",
    "parse_dates = ['order_purchase_timestamp', 'order_delivered_carrier_date', 'order_delivered_customer_date', 'order_estimated_delivery_date']\n",
    "orders_df = load_dataset(g_orders_dataset, parse_dates=parse_dates)\n",
    "payments_df = load_dataset(g_payments_dataset)\n",
    "orderitems_df = load_dataset(g_orderitems_dataset)\n",
    "cust_ord_df = orders_df.set_index('customer_id').join(customer_df.set_index('customer_id'), how=\"inner\").reset_index()\n",
    "cust_ord_df = cust_ord_df.set_index('order_id').join(orderitems_df.set_index('order_id'), how=\"inner\").reset_index()\n",
    "cust_ord_df['monetary_value'] = np.round(cust_ord_df['price'] + cust_ord_df['freight_value'], 0)\n",
    "cust_ord_df['order_date'] = cust_ord_df.order_purchase_timestamp.dt.date\n",
    "cust_ord_df['cancelled'] = 0.0\n",
    "cust_ord_df['cancelled'][cust_ord_df.order_status == 'canceled'] = 1.0\n",
    "customer_id_col='customer_unique_id'\n",
    "datetime_col='order_date'\n",
    "monetary_value_col='monetary_value' \n",
    "calibration_period_end = datetime(2018,4,1).date()\n",
    "observation_period_end = cust_ord_df.order_purchase_timestamp.max().date()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_datediffcolumn(df, col_dt1, col_dt2, colname):\n",
    "    df[colname] = np.round((df[col_dt1] - df[col_dt2]).dt.days, 0) * 1.0\n",
    "    #df[colname] = np.round((df[col_dt1] - df[col_dt2]) / np.timedelta64(1, 'M'), 0)\n",
    "    df[colname][df[colname].isna()] = 0.0\n",
    "    return df\n",
    "\n",
    "def get_calibration_holdout_data(df\n",
    "                               , customer_id_col='id'\n",
    "                               , datetime_col='date'\n",
    "                               , calibration_period_end=None\n",
    "                               , observation_period_end=None\n",
    "                               , monetary_value_col='value'\n",
    "                               , covariates=None):\n",
    "    allcols = [customer_id_col, datetime_col, monetary_value_col]\n",
    "    if covariates is not None:\n",
    "        allcols = allcols + covariates\n",
    "    df = df[allcols]\n",
    "    df['obs'] = 0\n",
    "    df['obs'][df[datetime_col] >= calibration_period_end] = 1\n",
    "    df['x'] = 1.0\n",
    "    sort_cols = [customer_id_col, 'obs', datetime_col]\n",
    "    agg_map = {'x':'count', monetary_value_col:'mean'}\n",
    "    if covariates is not None:\n",
    "        for covariate in covariates:\n",
    "            agg_map[covariate] = 'sum'\n",
    "    df = df.groupby([customer_id_col, 'obs', datetime_col]).agg(agg_map).reset_index() #.sort_values(sort_cols).groupby([customer_id_col, 'obs']).cumcount()+1\n",
    "    df['x'] = df.sort_values(sort_cols).groupby([customer_id_col, 'obs']).cumcount()+1    \n",
    "    df['x'] = df['x'] - 1.0\n",
    "    df['x'][df.obs == 1] += 1.0 \n",
    "    df['first'] = df[datetime_col]       \n",
    "    df['last'] = df[datetime_col]       \n",
    "    groupby_cols = [customer_id_col, 'obs']\n",
    "    all_cols = groupby_cols + [datetime_col, monetary_value_col, 'x', 'first', 'last'] + covariates\n",
    "    agg_map = {monetary_value_col:'mean', 'x':'max', 'first':'min', 'last':'max' }\n",
    "    if covariates is not None:\n",
    "        for covariate in covariates:\n",
    "            agg_map[covariate] = 'sum'\n",
    "    df = df.sort_values(sort_cols)[all_cols].groupby(groupby_cols).agg(agg_map).reset_index()\n",
    "    df['endobs'] = calibration_period_end    \n",
    "    df['endobs'][df.obs == 1] = observation_period_end\n",
    "    df = add_datediffcolumn(df, 'last', 'first', 't')\n",
    "    df = add_datediffcolumn(df, 'endobs', 'first', 'T')\n",
    "    df['T'][df.obs == 1] = np.round((observation_period_end - calibration_period_end).days, 0) * 1.0\n",
    "    cols = ['x', 't', 'T', monetary_value_col] + covariates\n",
    "    cal_df = df[df.obs == 0][[customer_id_col] + cols] \n",
    "    cal_df.columns = [customer_id_col] + ['{}_cal'.format(colname) for colname in cols]\n",
    "    hold_df = df[df.obs == 1][[customer_id_col] + cols] \n",
    "    hold_df.columns = [customer_id_col] + ['{}_holdout'.format(colname) for colname in cols]\n",
    "    df = cal_df.set_index(customer_id_col).join(hold_df.set_index(customer_id_col), how=\"left\").reset_index().fillna(0.0)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_calibration_holdout_data(cust_ord_df\n",
    "                                , customer_id_col=customer_id_col\n",
    "                                , datetime_col=datetime_col\n",
    "                                , calibration_period_end=calibration_period_end\n",
    "                                , observation_period_end=observation_period_end\n",
    "                                , monetary_value_col='monetary_value'\n",
    "                                , covariates=['cancelled'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x ==> number of repeat purchases\n",
    "# t ==> First purchase to last purchase\n",
    "# T ==> First purchase to end of observation period\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Regressors (Covariates) for location of 1st-stage prior, i.e. beta = [log(lambda), log(mu)]\n",
    "def set_regressors(data, covariates=[]):\n",
    "    data['intercept'] = 1.0\n",
    "    covariates = ['intercept'] + covariates\n",
    "    covars = np.matrix(data[covariates])\n",
    "    K = len(covariates)\n",
    "    return covariates, covars, K\n",
    "\n",
    "def get_diag(shape, val):\n",
    "    d = np.zeros(shape=shape)\n",
    "    np.fill_diagonal(d, val) \n",
    "    return d\n",
    "\n",
    "def get_map_from_array(x):\n",
    "    a_map = {}\n",
    "    count = 0\n",
    "    for val in x:\n",
    "        a_map[val] = count\n",
    "        count += 1\n",
    "    return a_map\n",
    "\n",
    "# set hyper priors \"log_lambda\", \"log_mu\"\n",
    "def set_hyperpriors(K):  \n",
    "    beta_0 = np.zeros(shape=(K, 2))\n",
    "    A_0 = get_diag(shape=(K, K), val=0.01) # diffuse precision matrix\n",
    "    # set diffuse hyper-parameters for 2nd-stage prior of gamma_0; follows defaults from rmultireg example\n",
    "    nu_00 = 3 + K  # 30\n",
    "    gamma_00 = get_diag(shape=(2, 2), val=nu_00) # diffuse precision matrix\n",
    "    hyper_prior = {'beta_0': beta_0, 'A_0':A_0, 'nu_00':nu_00, 'gamma_00':gamma_00}\n",
    "    return hyper_prior\n",
    "\n",
    "def draw_z(data, level_1, level_1_params_map):\n",
    "    tx = data['x_cal']\n",
    "    Tcal = data['T_cal']\n",
    "    p_lambda = level_1[level_1_params_map['lambda'], ]\n",
    "    p_mu = level_1[level_1_params_map['mu'], ]\n",
    "\n",
    "    mu_lam = p_mu + p_lambda\n",
    "    t_diff = Tcal - tx\n",
    "\n",
    "    prob = 1 / (1 + (p_mu / mu_lam) * (np.exp(mu_lam * t_diff) - 1))\n",
    "    z = (np.random.uniform(size=len(prob)) < prob)\n",
    "    z[z == True] = 1\n",
    "    z = z.astype(int)\n",
    "    return list(z.values)\n",
    "\n",
    "def draw_tau(data, level_1, level_1_params_map):\n",
    "    N = len(data)\n",
    "    tx = data['x_cal']\n",
    "    Tcal = data['T_cal']\n",
    "    p_lambda = level_1[level_1_params_map['lambda'], ]\n",
    "    p_mu = level_1[level_1_params_map['mu'], ]\n",
    "\n",
    "    mu_lam = p_mu + p_lambda\n",
    "    z = level_1[level_1_params_map['z'], ]\n",
    "\n",
    "    alive = (z == 1)\n",
    "    tau = np.zeros(shape=(N))\n",
    "\n",
    "    # Case: still alive - left truncated exponential distribution -> [T.cal, Inf]\n",
    "    if (np.sum(alive) > 0):\n",
    "        tau[alive] = Tcal[alive] + np.random.exponential(scale=1.0/p_mu[alive], size=np.sum(alive))\n",
    "\n",
    "    # Case: churned - double truncated exponential distribution -> [tx, T.cal]\n",
    "    if (np.sum(~alive) > 0):\n",
    "        mu_lam_tx = np.minimum(700, mu_lam[~alive] * tx[~alive])\n",
    "        mu_lam_Tcal = np.minimum(700, mu_lam[~alive] * Tcal[~alive])\n",
    "        rand = np.random.uniform(size=np.sum(~alive))        \n",
    "        tau[~alive] = (-1.0 * np.log((1.0 - rand) * np.exp(-1.0 * mu_lam_tx) + rand * np.exp((-1.0 * mu_lam_Tcal)))) / mu_lam[~alive]\n",
    "\n",
    "    return tau\n",
    "\n",
    "def draw_level_2(covars, level_1, level_1_params_map, hyper_prior):\n",
    "    # standard multi-variate normal regression update\n",
    "    # Ported from \n",
    "    # https://github.com/cran/bayesm/blob/master/src/rmultireg_rcpp.cpp\n",
    "    # Arguments:\n",
    "    #  Y is n x m matrix\n",
    "    #  X is n x k\n",
    "    #  Bbar is the prior mean of regression coefficients  (k x m)\n",
    "    #  A is prior precision matrix\n",
    "    #  nu, V are parameters for prior on Sigma\n",
    "\n",
    "    # Output: list of B, Sigma draws of matrix of coefficients and Sigma matrix\n",
    "\n",
    "    # Model: \n",
    "    #  Y=XB+U  cov(u_i) = Sigma\n",
    "    #  B is k x m matrix of coefficients\n",
    "\n",
    "    # Prior:  \n",
    "    #  beta|Sigma  ~ N(betabar,Sigma (x) A^-1)\n",
    "    #  betabar=vec(Bbar)\n",
    "    #  beta = vec(B) \n",
    "    #  Sigma ~ IW(nu,V) or Sigma^-1 ~ W(nu, V^-1)\n",
    "\n",
    "    Y = np.log(level_1[[level_1_params_map['lambda'], level_1_params_map['mu']],].T)\n",
    "    X = covars\n",
    "    Bbar = hyper_prior['beta_0']\n",
    "    A = hyper_prior['A_0']\n",
    "    nu = hyper_prior['nu_00']\n",
    "    V = hyper_prior['gamma_00']\n",
    "\n",
    "    n = Y.shape[0]\n",
    "    m = Y.shape[1]\n",
    "    k = X.shape[1]    \n",
    "\n",
    "    #first draw Sigma\n",
    "    RA = cholesky(A)\n",
    "    W = np.concatenate((X, RA), axis=0) \n",
    "    Z = np.concatenate((Y, RA*Bbar), axis=0)\n",
    "    # note:  Y,X,A,Bbar must be matrices!\n",
    "    IR = solve(np.triu(cholesky(W.T*W)), np.eye(k,k)) #trimatu interprets the matrix as upper triangular and makes solve more efficient\n",
    "    # W'W = R'R  &  (W'W)^-1 = IRIR'  -- this is the UL decomp!\n",
    "    Btilde = (IR*IR.T) * (W.T*Z);\n",
    "    # IRIR'(W'Z) = (X'X+A)^-1(X'Y + ABbar)\n",
    "    E = Z-W*Btilde;\n",
    "    S = E.T*E;\n",
    "    # E'E    \n",
    "    # compute the inverse of V+S\n",
    "    ucholinv = solve(np.triu(cholesky(V+S)), np.eye(m,m))\n",
    "    VSinv = ucholinv*ucholinv.T\n",
    "    W = wishart.rvs(df=nu+n, scale=VSinv)\n",
    "    IW = invwishart.rvs(df=nu+n, scale=VSinv)\n",
    "    C = cholesky(W).T\n",
    "    CI = solve(C, np.eye(W.shape[0], W.shape[1]))\n",
    "    # now draw B given Sigma\n",
    "    #   note beta ~ N(vec(Btilde),Sigma (x) Covxxa)\n",
    "    #       Cov=(X'X + A)^-1  = IR t(IR)  \n",
    "    #       Sigma=CICI'    \n",
    "    #       therefore, cov(beta)= Omega = CICI' (x) IR IR' = (CI (x) IR) (CI (x) IR)'\n",
    "    #  so to draw beta we do beta= vec(Btilde) +(CI (x) IR)vec(Z_mk)  \n",
    "    #       Z_mk is m x k matrix of N(0,1)\n",
    "    #  since vec(ABC) = (C' (x) A)vec(B), we have \n",
    "    #       B = Btilde + IR Z_mk CI'\n",
    "    samples = np.random.normal(size=k*m).reshape(k,m)\n",
    "    B = Btilde + IR*samples*CI.T\n",
    "    return {'beta': B.T, 'gamma':IW}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_level_1(data, covars, level_1, level_2):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  draw_level_1 <- function(data, covars, level_1, level_2) {\n",
    "    # sample (lambda, mu) given (z, tau, beta, gamma)\n",
    "    N <- nrow(data)\n",
    "    x <- data$x\n",
    "    Tcal <- data$T.cal\n",
    "    z <- level_1[\"z\", ]\n",
    "    tau <- level_1[\"tau\", ]\n",
    "    mvmean <- covars[, ] %*% t(level_2$beta)\n",
    "    gamma <- level_2$gamma\n",
    "    inv_gamma <- solve(gamma)\n",
    "\n",
    "    cur_lambda <- level_1[\"lambda\", ]\n",
    "    cur_mu <- level_1[\"mu\", ]\n",
    "\n",
    "    log_post <- function(log_theta) {\n",
    "      log_lambda <- log_theta[1, ]\n",
    "      log_mu <- log_theta[2, ]\n",
    "      diff_lambda <- log_lambda - mvmean[, 1]\n",
    "      diff_mu <- log_mu - mvmean[, 2]\n",
    "      likel <- x * log_lambda + (1 - z) * log_mu - (exp(log_lambda) + exp(log_mu)) * (z * Tcal + (1 - z) *\n",
    "        tau)\n",
    "      prior <- -0.5 * (diff_lambda ^ 2 * inv_gamma[1, 1] +\n",
    "                         2 * diff_lambda * diff_mu * inv_gamma[1, 2] +\n",
    "                         diff_mu ^ 2 * inv_gamma[2, 2])\n",
    "      post <- likel + prior\n",
    "      post[log_mu > 5] <- -Inf  # cap !!\n",
    "      return(post)\n",
    "    }\n",
    "\n",
    "    # current state\n",
    "    cur_log_theta <- rbind(log(cur_lambda), log(cur_mu))\n",
    "    cur_post <- log_post(cur_log_theta)\n",
    "\n",
    "    step <- function(cur_log_theta, cur_post) {\n",
    "      # new proposal\n",
    "      new_log_theta <- cur_log_theta + rbind(gamma[1, 1] * rt(N, df = 3), gamma[2, 2] * rt(n = N, df = 3))\n",
    "      new_log_theta[1, ] <- pmax(pmin(new_log_theta[1, ], 70), -70)\n",
    "      new_log_theta[2, ] <- pmax(pmin(new_log_theta[2, ], 70), -70)\n",
    "      new_post <- log_post(new_log_theta)\n",
    "\n",
    "      # accept/reject new proposal\n",
    "      mhratio <- exp(new_post - cur_post)\n",
    "      accepted <- mhratio > runif(n = N)\n",
    "\n",
    "      cur_log_theta[, accepted] <- new_log_theta[, accepted]\n",
    "      cur_post[accepted] <- new_post[accepted]\n",
    "\n",
    "      list(cur_log_theta = cur_log_theta, cur_post = cur_post)\n",
    "    }\n",
    "\n",
    "    iter <- 1  # how high do we need to set this? 1/5/10/100?\n",
    "    for (i in 1:iter) {\n",
    "      draw <- step(cur_log_theta, cur_post)\n",
    "      cur_log_theta <- draw$cur_log_theta\n",
    "      cur_post <- draw$cur_post\n",
    "    }\n",
    "    cur_theta <- exp(cur_log_theta)\n",
    "\n",
    "    return(list(lambda = cur_theta[1, ], mu = cur_theta[2, ]))\n",
    "  }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "covariates, covars, K = set_regressors(df, covariates=[\"cancelled_cal\"])\n",
    "hyper_prior = set_hyperpriors(K)\n",
    "\n",
    "# Prep code \n",
    "data = df\n",
    "LOG_LAMBDA = 0\n",
    "LOG_MU = 1\n",
    "nr_of_cust = len(data)\n",
    "nburnin = 70\n",
    "nsample = 30\n",
    "nskip = 10\n",
    "nr_of_draws = nburnin + nsample * nskip\n",
    "\n",
    "# The 4 is for \"lambda\", \"mu\", \"tau\", \"z\"\n",
    "level_1_params_map = get_map_from_array(['lambda', 'mu', 'tau', 'z'])\n",
    "level_1_draws = np.zeros(shape=(nr_of_draws, 4, nr_of_cust))\n",
    "\n",
    "level_2_draws = np.zeros(shape=(nr_of_draws, (2*K)+3))\n",
    "nm = ['log_lambda', 'log_mu']\n",
    "if (K > 1):\n",
    "    nm = ['{}_{}'.format(val2, val1) for val1 in covariates for val2 in nm]\n",
    "nm.extend(['var_log_lambda', 'cov_log_lambda_log_mu', 'var_log_mu'])\n",
    "level_2_params_map = get_map_from_array(nm)\n",
    "        \n",
    "## initialize parameters ##\n",
    "data['t_cal_tmp'] = data['t_cal']\n",
    "data['t_cal_tmp'] = data['t_cal']\n",
    "data['t_cal_tmp'][data.t_cal == 0] = data['T_cal'][data.t_cal == 0] \n",
    "level_1 = level_1_draws[1,]\n",
    "x_cal_mean = np.mean(data['x_cal'])\n",
    "t_cal_tmp_mean = np.mean(data['t_cal_tmp'])\n",
    "level_1[level_1_params_map['lambda'], ] = x_cal_mean/t_cal_tmp_mean\n",
    "level_1[level_1_params_map['mu'], ] = 1 / (data['t_cal'] + 0.5 / level_1[level_1_params_map['lambda'], ])\n",
    "    \n",
    "## run MCMC chain ##\n",
    "hyper_prior['beta_0'][0, LOG_LAMBDA] = np.log(np.mean(level_1[level_1_params_map['lambda'], ]))\n",
    "hyper_prior['beta_0'][0, LOG_MU] = np.log(np.mean(level_1[level_1_params_map['mu'], ]))\n",
    "    \n",
    "# draw individual-level parameters\n",
    "level_1[level_1_params_map['z'], ] = draw_z(data, level_1, level_1_params_map)\n",
    "level_1[level_1_params_map['tau'], ] = draw_tau(data, level_1, level_1_params_map)\n",
    "level_2 = draw_level_2(covars, level_1, level_1_params_map, hyper_prior)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'beta': matrix([[-9.03568939, -5.67186523],\n",
       "         [-8.34165125, -5.2365695 ]]),\n",
       " 'gamma': array([[7.69066869e-10, 2.20335565e-10],\n",
       "        [2.20335565e-10, 1.39621941e-06]])}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "level_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_chain(data, covariates, K, hyper_prior, nsample, nburnin, nskip):\n",
    "    ## initialize arrays for storing draws ##\n",
    "    LOG_LAMBDA = 0\n",
    "    LOG_MU = 1\n",
    "    nr_of_cust = len(data)\n",
    "    nr_of_draws = nburnin + nsample * nskip\n",
    "\n",
    "    # The 4 is for \"lambda\", \"mu\", \"tau\", \"z\"\n",
    "    level_1_params_map = get_map_from_array(['lambda', 'mu', 'tau', 'z'])\n",
    "    level_1_draws = np.zeros(shape=(nr_of_draws, 4, nr_of_cust))\n",
    "\n",
    "    level_2_draws = np.zeros(shape=(nr_of_draws, (2*K)+3))\n",
    "    nm = ['log_lambda', 'log_mu']\n",
    "    if (K > 1):\n",
    "        nm = ['{}_{}'.format(val2, val1) for val1 in covariates for val2 in nm]\n",
    "    nm.extend(['var_log_lambda', 'cov_log_lambda_log_mu', 'var_log_mu'])\n",
    "    level_2_params_map = get_map_from_array(nm)\n",
    "        \n",
    "    ## initialize parameters ##\n",
    "    data['t_cal_tmp'] = data['t_cal']\n",
    "    data['t_cal_tmp'] = data['t_cal']\n",
    "    data['t_cal_tmp'][data.t_cal == 0] = data['T_cal'][data.t_cal == 0] \n",
    "    level_1 = level_1_draws[1,]\n",
    "    x_cal_mean = np.mean(data['x_cal'])\n",
    "    t_cal_tmp_mean = np.mean(data['t_cal_tmp'])\n",
    "    level_1[level_1_params_map['lambda'], ] = x_cal_mean/t_cal_tmp_mean\n",
    "    level_1[level_1_params_map['mu'], ] = 1 / (data['t_cal'] + 0.5 / level_1[level_1_params_map['lambda'], ])\n",
    "    \n",
    "    ## run MCMC chain ##\n",
    "    hyper_prior['beta_0'][0, LOG_LAMBDA] = np.log(np.mean(level_1[level_1_params_map['lambda'], ]))\n",
    "    hyper_prior['beta_0'][0, LOG_MU] = np.log(np.mean(level_1[level_1_params_map['mu'], ]))\n",
    "    \n",
    "    for i in range(0, nr_of_draws):\n",
    "        # draw individual-level parameters\n",
    "        level_1[level_1_params_map['z'], ] = draw_z(data, level_1, level_1_params_map)\n",
    "        level_1[level_1_params_map['tau'], ] = draw_tau(data, level_1, level_1_params_map)\n",
    "\n",
    "        level_2 = draw_level_2(covars, level_1, hyper_prior)\n",
    "        \n",
    "        nk = int(round((i - nburnin) / nskip))\n",
    "        if (i > nskip and floor(nk) == nk and nk > 0):\n",
    "            #Store\n",
    "            \n",
    "\n",
    "    for (step in 1:(burnin + mcmc)) {\n",
    "      if (step %% trace == 0)\n",
    "        cat(\"chain:\", chain_id, \"step:\", step, \"of\", (burnin + mcmc), \"\\n\")\n",
    "\n",
    "      # draw individual-level parameters\n",
    "      level_1[\"z\", ] <- draw_z(data, level_1)\n",
    "      level_1[\"tau\", ] <- draw_tau(data, level_1)\n",
    "\n",
    "      level_2 <- draw_level_2(covars, level_1, hyper_prior)\n",
    "\n",
    "      draw <- draw_level_1(data, covars, level_1, level_2)\n",
    "      level_1[\"lambda\", ] <- draw$lambda\n",
    "      level_1[\"mu\", ] <- draw$mu\n",
    "\n",
    "      # store\n",
    "      if ( (step - burnin) > 0 & (step - 1 - burnin) %% thin == 0) {\n",
    "        idx <- (step - 1 - burnin) %/% thin + 1\n",
    "        level_1_draws[idx, , ] <- level_1 # nolint\n",
    "        level_2_draws[idx, ] <- c(level_2$beta, level_2$gamma[1, 1], level_2$gamma[1, 2], level_2$gamma[2,\n",
    "          2])\n",
    "      }\n",
    "    }\n",
    "\n",
    "    # convert MCMC draws into coda::mcmc objects\n",
    "    return(list(\n",
    "      \"level_1\" = lapply(1:nr_of_cust,\n",
    "                         function(i) mcmc(level_1_draws[, , i], start = burnin, thin = thin)), # nolint\n",
    "      \"level_2\" = mcmc(level_2_draws, start = burnin, thin = thin)))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_single_chain(df, K=K, hyper_prior=hyper_prior, nsample, nburnin, nskip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_1[level_1_params_map['z'], ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(data)\n",
    "tx = data['x_cal']\n",
    "Tcal = data['T_cal']\n",
    "p_lambda = level_1[level_1_params_map['lambda'], ]\n",
    "p_mu = level_1[level_1_params_map['mu'], ]\n",
    "\n",
    "mu_lam = p_mu + p_lambda\n",
    "z = level_1[level_1_params_map['z'], ]\n",
    "\n",
    "alive = (z == 1)\n",
    "tau = np.zeros(shape=(N))\n",
    "\n",
    "tau[alive] = Tcal[alive] + np.random.exponential(scale=1.0/p_mu[alive], size=np.sum(alive))\n",
    "\n",
    "mu_lam_tx = np.minimum(700, mu_lam[~alive] * tx[~alive])\n",
    "mu_lam_Tcal = np.minimum(700, mu_lam[~alive] * Tcal[~alive])\n",
    "rand = np.random.uniform(size=np.sum(~alive))\n",
    "tau[~alive] = (-1.0 * np.log((1.0 - rand) * np.exp(-1.0 * mu_lam_tx) + rand * np.exp((-1.0 * mu_lam_Tcal)))) / mu_lam[~alive]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1.0/p_mu[alive]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_prior['beta_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_1[level_1_params_map['mu'], ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(data['x_cal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['t_cal_tmp'] = data['t_cal']\n",
    "data['t_cal_tmp'] = data['t_cal']\n",
    "data['t_cal_tmp'][data.t_cal == 0] = data['T_cal'][data.t_cal == 0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.t_cal > 0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

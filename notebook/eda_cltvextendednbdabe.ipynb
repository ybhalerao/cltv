{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import floor, ceil\n",
    "from numpy.linalg import cholesky, inv, solve\n",
    "from scipy.stats import wishart, invwishart\n",
    "from lifetimes import BetaGeoFitter, GammaGammaFitter\n",
    "from lifetimes.utils import calibration_and_holdout_data, summary_data_from_transaction_data\n",
    "from lifetimes.plotting import plot_calibration_purchases_vs_holdout_purchases, plot_period_transactions\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(datafile, parse_dates=None):\n",
    "    df = pd.read_csv(datafile, delimiter=',', parse_dates=parse_dates)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_datafolder = '/development/data'\n",
    "g_customer_dataset = '{}/olist_customers_dataset.csv'.format(g_datafolder)\n",
    "g_orders_dataset = '{}/olist_orders_dataset.csv'.format(g_datafolder)\n",
    "g_payments_dataset = '{}/olist_order_payments_dataset.csv'.format(g_datafolder)\n",
    "g_orderitems_dataset = '{}/olist_order_items_dataset.csv'.format(g_datafolder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "customer_df = load_dataset(g_customer_dataset)\n",
    "parse_dates = ['order_purchase_timestamp', 'order_delivered_carrier_date', 'order_delivered_customer_date', 'order_estimated_delivery_date']\n",
    "orders_df = load_dataset(g_orders_dataset, parse_dates=parse_dates)\n",
    "payments_df = load_dataset(g_payments_dataset)\n",
    "orderitems_df = load_dataset(g_orderitems_dataset)\n",
    "cust_ord_df = orders_df.set_index('customer_id').join(customer_df.set_index('customer_id'), how=\"inner\").reset_index()\n",
    "cust_ord_df = cust_ord_df.set_index('order_id').join(orderitems_df.set_index('order_id'), how=\"inner\").reset_index()\n",
    "cust_ord_df['monetary_value'] = np.round(cust_ord_df['price'] + cust_ord_df['freight_value'], 0)\n",
    "cust_ord_df['order_date'] = cust_ord_df.order_purchase_timestamp.dt.date\n",
    "cust_ord_df['cancelled'] = 0.0\n",
    "cust_ord_df['cancelled'][cust_ord_df.order_status == 'canceled'] = 1.0\n",
    "customer_id_col='customer_unique_id'\n",
    "datetime_col='order_date'\n",
    "monetary_value_col='monetary_value' \n",
    "calibration_period_end = datetime(2018,4,1).date()\n",
    "observation_period_end = cust_ord_df.order_purchase_timestamp.max().date()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_datediffcolumn(df, col_dt1, col_dt2, colname):\n",
    "    df[colname] = np.round((df[col_dt1] - df[col_dt2]).dt.days, 0) * 1.0\n",
    "    #df[colname] = np.round((df[col_dt1] - df[col_dt2]) / np.timedelta64(1, 'M'), 0)\n",
    "    df[colname][df[colname].isna()] = 0.0\n",
    "    return df\n",
    "\n",
    "def get_calibration_holdout_data(df\n",
    "                               , customer_id_col='id'\n",
    "                               , datetime_col='date'\n",
    "                               , calibration_period_end=None\n",
    "                               , observation_period_end=None\n",
    "                               , monetary_value_col='value'\n",
    "                               , covariates=None):\n",
    "    allcols = [customer_id_col, datetime_col, monetary_value_col]\n",
    "    if covariates is not None:\n",
    "        allcols = allcols + covariates\n",
    "    df = df[allcols]\n",
    "    df['obs'] = 0\n",
    "    df['obs'][df[datetime_col] >= calibration_period_end] = 1\n",
    "    df['x'] = 1.0\n",
    "    sort_cols = [customer_id_col, 'obs', datetime_col]\n",
    "    agg_map = {'x':'count', monetary_value_col:'mean'}\n",
    "    if covariates is not None:\n",
    "        for covariate in covariates:\n",
    "            agg_map[covariate] = 'sum'\n",
    "    df = df.groupby([customer_id_col, 'obs', datetime_col]).agg(agg_map).reset_index() #.sort_values(sort_cols).groupby([customer_id_col, 'obs']).cumcount()+1\n",
    "    df['x'] = df.sort_values(sort_cols).groupby([customer_id_col, 'obs']).cumcount()+1    \n",
    "    df['x'] = df['x'] - 1.0\n",
    "    df['x'][df.obs == 1] += 1.0 \n",
    "    df['first'] = df[datetime_col]       \n",
    "    df['last'] = df[datetime_col]       \n",
    "    groupby_cols = [customer_id_col, 'obs']\n",
    "    all_cols = groupby_cols + [datetime_col, monetary_value_col, 'x', 'first', 'last'] + covariates\n",
    "    agg_map = {monetary_value_col:'mean', 'x':'max', 'first':'min', 'last':'max' }\n",
    "    if covariates is not None:\n",
    "        for covariate in covariates:\n",
    "            agg_map[covariate] = 'sum'\n",
    "    df = df.sort_values(sort_cols)[all_cols].groupby(groupby_cols).agg(agg_map).reset_index()\n",
    "    df['endobs'] = calibration_period_end    \n",
    "    df['endobs'][df.obs == 1] = observation_period_end\n",
    "    df = add_datediffcolumn(df, 'last', 'first', 't')\n",
    "    df = add_datediffcolumn(df, 'endobs', 'first', 'T')\n",
    "    df['T'][df.obs == 1] = np.round((observation_period_end - calibration_period_end).days, 0) * 1.0\n",
    "    cols = ['x', 't', 'T', monetary_value_col] + covariates\n",
    "    cal_df = df[df.obs == 0][[customer_id_col] + cols] \n",
    "    cal_df.columns = [customer_id_col] + ['{}_cal'.format(colname) for colname in cols]\n",
    "    hold_df = df[df.obs == 1][[customer_id_col] + cols] \n",
    "    hold_df.columns = [customer_id_col] + ['{}_holdout'.format(colname) for colname in cols]\n",
    "    df = cal_df.set_index(customer_id_col).join(hold_df.set_index(customer_id_col), how=\"left\").reset_index().fillna(0.0)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_calibration_holdout_data(cust_ord_df\n",
    "                                , customer_id_col=customer_id_col\n",
    "                                , datetime_col=datetime_col\n",
    "                                , calibration_period_end=calibration_period_end\n",
    "                                , observation_period_end=observation_period_end\n",
    "                                , monetary_value_col='monetary_value'\n",
    "                                , covariates=['cancelled'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x ==> number of repeat purchases\n",
    "# t ==> First purchase to last purchase\n",
    "# T ==> First purchase to end of observation period\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Regressors (Covariates) for location of 1st-stage prior, i.e. beta = [log(lambda), log(mu)]\n",
    "def set_regressors(data, covariates=[]):\n",
    "    data['intercept'] = 1.0\n",
    "    covariates = ['intercept'] + covariates\n",
    "    covars = np.matrix(data[covariates])\n",
    "    K = len(covariates)\n",
    "    return covariates, covars, K\n",
    "\n",
    "def get_diag(shape, val):\n",
    "    d = np.zeros(shape=shape)\n",
    "    np.fill_diagonal(d, val) \n",
    "    return d\n",
    "\n",
    "def get_map_from_array(x):\n",
    "    a_map = {}\n",
    "    count = 0\n",
    "    for val in x:\n",
    "        a_map[val] = count\n",
    "        count += 1\n",
    "    return a_map\n",
    "\n",
    "# set hyper priors \"log_lambda\", \"log_mu\"\n",
    "def set_hyperpriors(K):  \n",
    "    beta_0 = np.zeros(shape=(K, 2))\n",
    "    A_0 = get_diag(shape=(K, K), val=0.01) # diffuse precision matrix\n",
    "    # set diffuse hyper-parameters for 2nd-stage prior of gamma_0; follows defaults from rmultireg example\n",
    "    nu_00 = 3 + K  # 30\n",
    "    gamma_00 = get_diag(shape=(2, 2), val=nu_00) # diffuse precision matrix\n",
    "    hyper_prior = {'beta_0': beta_0, 'A_0':A_0, 'nu_00':nu_00, 'gamma_00':gamma_00}\n",
    "    return hyper_prior\n",
    "\n",
    "def draw_z(data, level_1, level_1_params_map):\n",
    "    tx = data['x_cal']\n",
    "    Tcal = data['T_cal']\n",
    "    p_lambda = level_1[level_1_params_map['lambda'], ]\n",
    "    p_mu = level_1[level_1_params_map['mu'], ]\n",
    "\n",
    "    mu_lam = p_mu + p_lambda\n",
    "    t_diff = Tcal - tx\n",
    "\n",
    "    prob = 1 / (1 + (p_mu / mu_lam) * (np.exp(mu_lam * t_diff) - 1))\n",
    "    z = (np.random.uniform(size=len(prob)) < prob)\n",
    "    z[z == True] = 1\n",
    "    z = z.astype(int)\n",
    "    return list(z.values)\n",
    "\n",
    "def draw_tau(data, level_1, level_1_params_map):\n",
    "    N = len(data)\n",
    "    tx = data['x_cal']\n",
    "    Tcal = data['T_cal']\n",
    "    p_lambda = level_1[level_1_params_map['lambda'], ]\n",
    "    p_mu = level_1[level_1_params_map['mu'], ]\n",
    "\n",
    "    mu_lam = p_mu + p_lambda\n",
    "    z = level_1[level_1_params_map['z'], ]\n",
    "\n",
    "    alive = (z == 1)\n",
    "    tau = np.zeros(shape=(N))\n",
    "\n",
    "    # Case: still alive - left truncated exponential distribution -> [T.cal, Inf]\n",
    "    if (np.sum(alive) > 0):\n",
    "        tau[alive] = Tcal[alive] + np.random.exponential(scale=1.0/p_mu[alive], size=np.sum(alive))\n",
    "\n",
    "    # Case: churned - double truncated exponential distribution -> [tx, T.cal]\n",
    "    if (np.sum(~alive) > 0):\n",
    "        mu_lam_tx = np.minimum(700, mu_lam[~alive] * tx[~alive])\n",
    "        mu_lam_Tcal = np.minimum(700, mu_lam[~alive] * Tcal[~alive])\n",
    "        rand = np.random.uniform(size=np.sum(~alive))        \n",
    "        tau[~alive] = (-1.0 * np.log((1.0 - rand) * np.exp(-1.0 * mu_lam_tx) + rand * np.exp((-1.0 * mu_lam_Tcal)))) / mu_lam[~alive]\n",
    "\n",
    "    return tau\n",
    "\n",
    "def draw_wishart(df, scale):\n",
    "    W = wishart.rvs(df, scale)\n",
    "    IW = invwishart.rvs(df, scale)\n",
    "    C = cholesky(W).T\n",
    "    CI = solve(C, np.eye(W.shape[0], W.shape[1]))\n",
    "    return W, IW, C, CI\n",
    "\n",
    "def draw_level_2(covars, level_1, level_1_params_map, hyper_prior):\n",
    "    # standard multi-variate normal regression update\n",
    "    # Ported from \n",
    "    # https://github.com/cran/bayesm/blob/master/src/rmultireg_rcpp.cpp\n",
    "    # Arguments:\n",
    "    #  Y is n x m matrix\n",
    "    #  X is n x k\n",
    "    #  Bbar is the prior mean of regression coefficients  (k x m)\n",
    "    #  A is prior precision matrix\n",
    "    #  nu, V are parameters for prior on Sigma\n",
    "\n",
    "    # Output: list of B, Sigma draws of matrix of coefficients and Sigma matrix\n",
    "\n",
    "    # Model: \n",
    "    #  Y=XB+U  cov(u_i) = Sigma\n",
    "    #  B is k x m matrix of coefficients\n",
    "\n",
    "    # Prior:  \n",
    "    #  beta|Sigma  ~ N(betabar,Sigma (x) A^-1)\n",
    "    #  betabar=vec(Bbar)\n",
    "    #  beta = vec(B) \n",
    "    #  Sigma ~ IW(nu,V) or Sigma^-1 ~ W(nu, V^-1)\n",
    "\n",
    "    Y = np.log(level_1[[level_1_params_map['lambda'], level_1_params_map['mu']],].T)\n",
    "    X = covars\n",
    "    Bbar = hyper_prior['beta_0']\n",
    "    A = hyper_prior['A_0']\n",
    "    nu = hyper_prior['nu_00']\n",
    "    V = hyper_prior['gamma_00']\n",
    "\n",
    "    n = Y.shape[0]\n",
    "    m = Y.shape[1]\n",
    "    k = X.shape[1]    \n",
    "\n",
    "    #first draw Sigma\n",
    "    RA = cholesky(A)\n",
    "    W = np.concatenate((X, RA), axis=0) \n",
    "    Z = np.concatenate((Y, RA*Bbar), axis=0)\n",
    "    # note:  Y,X,A,Bbar must be matrices!\n",
    "    IR = solve(np.triu(cholesky(W.T*W)), np.eye(k,k)) #trimatu interprets the matrix as upper triangular and makes solve more efficient\n",
    "    # W'W = R'R  &  (W'W)^-1 = IRIR'  -- this is the UL decomp!\n",
    "    Btilde = (IR*IR.T) * (W.T*Z);\n",
    "    # IRIR'(W'Z) = (X'X+A)^-1(X'Y + ABbar)\n",
    "    E = Z-W*Btilde;\n",
    "    S = E.T*E;\n",
    "    # E'E    \n",
    "    # compute the inverse of V+S\n",
    "    ucholinv = solve(np.triu(cholesky(V+S)), np.eye(m,m))\n",
    "    VSinv = ucholinv*ucholinv.T\n",
    "    W, IW, C, CI = draw_wishart(df=nu+n, scale=VSinv)    \n",
    "    # now draw B given Sigma\n",
    "    #   note beta ~ N(vec(Btilde),Sigma (x) Covxxa)\n",
    "    #       Cov=(X'X + A)^-1  = IR t(IR)  \n",
    "    #       Sigma=CICI'    \n",
    "    #       therefore, cov(beta)= Omega = CICI' (x) IR IR' = (CI (x) IR) (CI (x) IR)'\n",
    "    #  so to draw beta we do beta= vec(Btilde) +(CI (x) IR)vec(Z_mk)  \n",
    "    #       Z_mk is m x k matrix of N(0,1)\n",
    "    #  since vec(ABC) = (C' (x) A)vec(B), we have \n",
    "    #       B = Btilde + IR Z_mk CI'\n",
    "    samples = np.random.normal(size=k*m).reshape(k,m)\n",
    "    B = Btilde + IR*samples*CI.T\n",
    "    return {'beta': B.T, 'gamma':IW}\n",
    "\n",
    "def log_post(log_theta, mvmean, x, z, Tcal, tau, inv_gamma):\n",
    "    log_lambda = log_theta[0,:]\n",
    "    log_mu = log_theta[1,:]\n",
    "    diff_lambda = log_lambda - mvmean[:,0]\n",
    "    diff_mu = log_mu - mvmean[:,1]      \n",
    "    likel = x * log_lambda + (1.0 - z) * log_mu - (np.exp(log_lambda) + np.exp(log_mu)) * (z * Tcal + (1.0 - z) * tau)\n",
    "    prior = -0.5 * (diff_lambda ^ 2 * inv_gamma[1, 1] +\n",
    "                         2.0 * diff_lambda * diff_mu * inv_gamma[0, 1] +\n",
    "                         diff_mu ^ 2 * inv_gamma[1, 1])\n",
    "    post = likel + prior\n",
    "    post[log_mu > 5] = np.NINF  # cap !!\n",
    "    return post\n",
    "\n",
    "def step(cur_log_theta, cur_post, gamma, N, mvmean, x, z, Tcal, tau, inv_gamma):\n",
    "    a = gamma[0, 0] * np.random.standard_t(df=3, size=N)\n",
    "    b = gamma[1, 1] * np.random.standard_t(df=3, size=N)\n",
    "    new_log_theta = cur_log_theta + np.concatenate((a, b), axis=0)\n",
    "    new_log_theta[0,:] = np.maximum(np.minimum(new_log_theta[0,:], 70), -70)\n",
    "    new_log_theta[1,:] = np.maximum(np.minimum(new_log_theta[1,:], 70), -70)\n",
    "    new_post = log_post(new_log_theta, mvmean, x, z, Tcal, tau, inv_gamma)\n",
    "    \n",
    "    # accept/reject new proposal\n",
    "    mhratio = np.exp(new_post - cur_post)\n",
    "    accepted = mhratio > np.random.unifprm(size=N)\n",
    "    \n",
    "    return {'cur_log_theta':cur_log_theta, 'cur_post':cur_post}\n",
    "\n",
    "def draw_level_1(data, covars, level_1, level_1_params_map, level_2):\n",
    "    # sample (lambda, mu) given (z, tau, beta, gamma)\n",
    "    N = len(data)\n",
    "    x = data['x_cal']\n",
    "    Tcal = data['T_cal']\n",
    "    z = level_1[level_1_params_map['z'], ]\n",
    "    tau = level_1[level_1_params_map['tau'], ]\n",
    "    mvmean = np.matmul(covars, level_2['beta'])\n",
    "    gamma = level_2['gamma']\n",
    "    inv_gamma = inv(gamma)\n",
    "    \n",
    "    cur_lambda = level_1[level_1_params_map['lambda'], ]\n",
    "    cur_mu = level_1[level_1_params_map['mu'], ]\n",
    "\n",
    "    # current state\n",
    "    cur_log_theta = np.concatenate((np.log(cur_lambda), np.log(cur_mu)), axis=0)\n",
    "    cur_post = log_post(cur_log_theta, mvmean, x, z, Tcal, tau, inv_gamma)\n",
    "    \n",
    "    iter = 1  # how high do we need to set this? 1/5/10/100?\n",
    "    for i in range(0, iter):\n",
    "        draw = step(cur_log_theta, cur_post, gamma, N, mvmean, x, z, Tcal, tau, inv_gamma)\n",
    "        cur_log_theta = draw['cur_log_theta']\n",
    "        cur_post = draw['cur_post']\n",
    "\n",
    "    cur_theta = np.exp(cur_log_theta)\n",
    "\n",
    "    return {'lambda':cur_theta[0,:], 'mu':cur_theta[1,:]}\n",
    "\n",
    "def run_single_chain(data, covariates, K, hyper_prior, nsample, nburnin, nskip):\n",
    "    ## initialize arrays for storing draws ##\n",
    "    LOG_LAMBDA = 0\n",
    "    LOG_MU = 1\n",
    "    nr_of_cust = len(data)\n",
    "    nr_of_draws = nburnin + nsample * nskip\n",
    "\n",
    "    # The 4 is for \"lambda\", \"mu\", \"tau\", \"z\"\n",
    "    level_1_params_map = get_map_from_array(['lambda', 'mu', 'tau', 'z'])\n",
    "    level_1_draws = np.zeros(shape=(nr_of_draws, 4, nr_of_cust))\n",
    "\n",
    "    level_2_draws = np.zeros(shape=(nr_of_draws, (2*K)+3))\n",
    "    nm = ['log_lambda', 'log_mu']\n",
    "    if (K > 1):\n",
    "        nm = ['{}_{}'.format(val2, val1) for val1 in covariates for val2 in nm]\n",
    "    nm.extend(['var_log_lambda', 'cov_log_lambda_log_mu', 'var_log_mu'])\n",
    "    level_2_params_map = get_map_from_array(nm)\n",
    "        \n",
    "    ## initialize parameters ##\n",
    "    data['t_cal_tmp'] = data['t_cal']\n",
    "    data['t_cal_tmp'] = data['t_cal']\n",
    "    data['t_cal_tmp'][data.t_cal == 0] = data['T_cal'][data.t_cal == 0] \n",
    "    level_1 = level_1_draws[1,]\n",
    "    x_cal_mean = np.mean(data['x_cal'])\n",
    "    t_cal_tmp_mean = np.mean(data['t_cal_tmp'])\n",
    "    level_1[level_1_params_map['lambda'], ] = x_cal_mean/t_cal_tmp_mean\n",
    "    level_1[level_1_params_map['mu'], ] = 1 / (data['t_cal'] + 0.5 / level_1[level_1_params_map['lambda'], ])\n",
    "    \n",
    "    ## run MCMC chain ##\n",
    "    hyper_prior['beta_0'][0, LOG_LAMBDA] = np.log(np.mean(level_1[level_1_params_map['lambda'], ]))\n",
    "    hyper_prior['beta_0'][0, LOG_MU] = np.log(np.mean(level_1[level_1_params_map['mu'], ]))\n",
    "    \n",
    "    for i in range(0, nr_of_draws):\n",
    "        # draw individual-level parameters\n",
    "        level_1[level_1_params_map['z'], ] = draw_z(data, level_1, level_1_params_map)\n",
    "        level_1[level_1_params_map['tau'], ] = draw_tau(data, level_1, level_1_params_map)\n",
    "\n",
    "        level_2 = draw_level_2(covars, level_1, level_1_params_map, hyper_prior)\n",
    "        \n",
    "        nk = int(round((i - nburnin) / nskip))\n",
    "        if (i > nskip and floor(nk) == nk and nk > 0):\n",
    "            #Store\n",
    "            level_1_draws[nk,:,:] = level_1 # nolint\n",
    "            level_2_draws[nk,:] = list(np.array(beta.T).reshape(-1)) + [level_2['gamma'][0, 0], level_2['gamma'][0, 1], level_2['gamma'][1,1]]\n",
    "        if (i % 100) == 0:\n",
    "            print('draw: {}'.format(i))\n",
    "            \n",
    "    return {\"level_1\":level_1_draws, \"level_2\":level_2_draws}    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "draw: 0\n",
      "draw: 10\n",
      "draw: 20\n",
      "draw: 30\n",
      "draw: 40\n",
      "draw: 50\n",
      "draw: 60\n",
      "draw: 70\n",
      "draw: 80\n",
      "draw: 90\n",
      "draw: 100\n",
      "draw: 110\n",
      "draw: 120\n",
      "draw: 130\n",
      "draw: 140\n",
      "draw: 150\n",
      "draw: 160\n",
      "draw: 170\n",
      "draw: 180\n",
      "draw: 190\n",
      "draw: 200\n",
      "draw: 210\n",
      "draw: 220\n",
      "draw: 230\n",
      "draw: 240\n",
      "draw: 250\n",
      "draw: 260\n",
      "draw: 270\n",
      "draw: 280\n",
      "draw: 290\n",
      "draw: 300\n",
      "draw: 310\n",
      "draw: 320\n",
      "draw: 330\n",
      "draw: 340\n",
      "draw: 350\n",
      "draw: 360\n"
     ]
    }
   ],
   "source": [
    "# Main routine\n",
    "covariates, covars, K = set_regressors(df, covariates=[\"cancelled_cal\"])\n",
    "hyper_prior = set_hyperpriors(K)\n",
    "draws = run_single_chain(df, covariates=covariates, K=K, hyper_prior=hyper_prior, nsample=nsample, nburnin=nburnin, nskip=nskip)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(370, 4, 63881)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "draws['level_1'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(370, 7)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "draws['level_2'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "covariates, covars, K = set_regressors(df, covariates=[\"cancelled_cal\"])\n",
    "hyper_prior = set_hyperpriors(K)\n",
    "\n",
    "# Prep code \n",
    "data = df\n",
    "LOG_LAMBDA = 0\n",
    "LOG_MU = 1\n",
    "nr_of_cust = len(data)\n",
    "nburnin = 70\n",
    "nsample = 30\n",
    "nskip = 10\n",
    "nr_of_draws = nburnin + nsample * nskip\n",
    "\n",
    "# The 4 is for \"lambda\", \"mu\", \"tau\", \"z\"\n",
    "level_1_params_map = get_map_from_array(['lambda', 'mu', 'tau', 'z'])\n",
    "level_1_draws = np.zeros(shape=(nr_of_draws, 4, nr_of_cust))\n",
    "\n",
    "level_2_draws = np.zeros(shape=(nr_of_draws, (2*K)+3))\n",
    "nm = ['log_lambda', 'log_mu']\n",
    "if (K > 1):\n",
    "    nm = ['{}_{}'.format(val2, val1) for val1 in covariates for val2 in nm]\n",
    "nm.extend(['var_log_lambda', 'cov_log_lambda_log_mu', 'var_log_mu'])\n",
    "level_2_params_map = get_map_from_array(nm)\n",
    "        \n",
    "## initialize parameters ##\n",
    "data['t_cal_tmp'] = data['t_cal']\n",
    "data['t_cal_tmp'] = data['t_cal']\n",
    "data['t_cal_tmp'][data.t_cal == 0] = data['T_cal'][data.t_cal == 0] \n",
    "level_1 = level_1_draws[1,]\n",
    "x_cal_mean = np.mean(data['x_cal'])\n",
    "t_cal_tmp_mean = np.mean(data['t_cal_tmp'])\n",
    "level_1[level_1_params_map['lambda'], ] = x_cal_mean/t_cal_tmp_mean\n",
    "level_1[level_1_params_map['mu'], ] = 1 / (data['t_cal'] + 0.5 / level_1[level_1_params_map['lambda'], ])\n",
    "    \n",
    "## run MCMC chain ##\n",
    "hyper_prior['beta_0'][0, LOG_LAMBDA] = np.log(np.mean(level_1[level_1_params_map['lambda'], ]))\n",
    "hyper_prior['beta_0'][0, LOG_MU] = np.log(np.mean(level_1[level_1_params_map['mu'], ]))\n",
    "    \n",
    "# draw individual-level parameters\n",
    "level_1[level_1_params_map['z'], ] = draw_z(data, level_1, level_1_params_map)\n",
    "level_1[level_1_params_map['tau'], ] = draw_tau(data, level_1, level_1_params_map)\n",
    "level_2 = draw_level_2(covars, level_1, level_1_params_map, hyper_prior)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "draw: 0\n",
      "draw: 10\n",
      "draw: 20\n",
      "draw: 30\n",
      "draw: 40\n",
      "draw: 50\n",
      "draw: 60\n",
      "draw: 70\n",
      "draw: 80\n",
      "draw: 90\n",
      "draw: 100\n",
      "draw: 110\n",
      "draw: 120\n",
      "draw: 130\n",
      "draw: 140\n",
      "draw: 150\n",
      "draw: 160\n",
      "draw: 170\n",
      "draw: 180\n",
      "draw: 190\n",
      "draw: 200\n",
      "draw: 210\n",
      "draw: 220\n",
      "draw: 230\n",
      "draw: 240\n",
      "draw: 250\n",
      "draw: 260\n",
      "draw: 270\n",
      "draw: 280\n",
      "draw: 290\n",
      "draw: 300\n",
      "draw: 310\n",
      "draw: 320\n",
      "draw: 330\n",
      "draw: 340\n",
      "draw: 350\n",
      "draw: 360\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'level_1': array([[[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00]],\n",
       " \n",
       "        [[1.19238170e-04, 1.19238170e-04, 1.19238170e-04, ...,\n",
       "          1.19238170e-04, 1.19238170e-04, 1.19238170e-04],\n",
       "         [2.38476341e-04, 2.38476341e-04, 2.38476341e-04, ...,\n",
       "          2.38476341e-04, 2.38476341e-04, 2.38476341e-04],\n",
       "         [1.05226389e+04, 4.74334369e+01, 3.36677370e+02, ...,\n",
       "          2.16144891e+03, 1.71518736e+03, 5.10626305e+03],\n",
       "         [1.00000000e+00, 0.00000000e+00, 1.00000000e+00, ...,\n",
       "          1.00000000e+00, 1.00000000e+00, 1.00000000e+00]],\n",
       " \n",
       "        [[1.19238170e-04, 1.19238170e-04, 1.19238170e-04, ...,\n",
       "          1.19238170e-04, 1.19238170e-04, 1.19238170e-04],\n",
       "         [2.38476341e-04, 2.38476341e-04, 2.38476341e-04, ...,\n",
       "          2.38476341e-04, 2.38476341e-04, 2.38476341e-04],\n",
       "         [7.71085914e+03, 7.54671625e+02, 4.07160608e+02, ...,\n",
       "          4.29971689e+02, 6.94247176e+03, 5.36465114e+02],\n",
       "         [1.00000000e+00, 1.00000000e+00, 1.00000000e+00, ...,\n",
       "          1.00000000e+00, 1.00000000e+00, 1.00000000e+00]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00]],\n",
       " \n",
       "        [[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00]],\n",
       " \n",
       "        [[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00]]]),\n",
       " 'level_2': array([[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [-2.22787900e+00, -2.57409600e+00,  3.38915700e-03, ...,\n",
       "          7.68284846e-10, -1.32896473e-10,  1.41685888e-06],\n",
       "        [-2.22787900e+00, -2.57409600e+00,  3.38915700e-03, ...,\n",
       "          7.62684736e-10, -3.05165135e-10,  1.41539042e-06],\n",
       "        ...,\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00]])}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_single_chain(df, covariates=covariates, K=K, hyper_prior=hyper_prior, nsample=nsample, nburnin=nburnin, nskip=nskip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_1[level_1_params_map['z'], ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(data)\n",
    "tx = data['x_cal']\n",
    "Tcal = data['T_cal']\n",
    "p_lambda = level_1[level_1_params_map['lambda'], ]\n",
    "p_mu = level_1[level_1_params_map['mu'], ]\n",
    "\n",
    "mu_lam = p_mu + p_lambda\n",
    "z = level_1[level_1_params_map['z'], ]\n",
    "\n",
    "alive = (z == 1)\n",
    "tau = np.zeros(shape=(N))\n",
    "\n",
    "tau[alive] = Tcal[alive] + np.random.exponential(scale=1.0/p_mu[alive], size=np.sum(alive))\n",
    "\n",
    "mu_lam_tx = np.minimum(700, mu_lam[~alive] * tx[~alive])\n",
    "mu_lam_Tcal = np.minimum(700, mu_lam[~alive] * Tcal[~alive])\n",
    "rand = np.random.uniform(size=np.sum(~alive))\n",
    "tau[~alive] = (-1.0 * np.log((1.0 - rand) * np.exp(-1.0 * mu_lam_tx) + rand * np.exp((-1.0 * mu_lam_Tcal)))) / mu_lam[~alive]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1.0/p_mu[alive]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_prior['beta_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_1[level_1_params_map['mu'], ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(data['x_cal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['t_cal_tmp'] = data['t_cal']\n",
    "data['t_cal_tmp'] = data['t_cal']\n",
    "data['t_cal_tmp'][data.t_cal == 0] = data['T_cal'][data.t_cal == 0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.t_cal > 0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

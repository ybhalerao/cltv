{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import floor, ceil\n",
    "from numpy.linalg import cholesky, inv, solve \n",
    "from scipy.linalg import cho_solve\n",
    "from scipy.stats import wishart, invwishart, gamma\n",
    "from lifetimes import BetaGeoFitter, GammaGammaFitter\n",
    "from lifetimes.utils import calibration_and_holdout_data, summary_data_from_transaction_data\n",
    "from lifetimes.plotting import plot_calibration_purchases_vs_holdout_purchases, plot_period_transactions\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(datafile, parse_dates=None):\n",
    "    df = pd.read_csv(datafile, delimiter=',', parse_dates=parse_dates)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x ==> number of repeat purchases\n",
    "# t ==> First purchase to last purchase\n",
    "# T ==> First purchase to end of observation period\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Regressors (Covariates) for location of 1st-stage prior, i.e. beta = [log(lambda), log(mu)]\n",
    "def set_regressors(data, covariates=[]):\n",
    "    data['intercept'] = 1.0\n",
    "    covariates = ['intercept'] + covariates\n",
    "    covars = np.matrix(data[covariates])\n",
    "    K = len(covariates)\n",
    "    return covariates, covars, K\n",
    "\n",
    "def get_diag(shape, val):\n",
    "    d = np.zeros(shape=shape)\n",
    "    np.fill_diagonal(d, val) \n",
    "    return d\n",
    "\n",
    "def get_map_from_array(x):\n",
    "    a_map = {}\n",
    "    count = 0\n",
    "    for val in x:\n",
    "        a_map[val] = count\n",
    "        count += 1\n",
    "    return a_map\n",
    "\n",
    "# set hyper priors \"log_lambda\", \"log_mu\"\n",
    "def set_hyperpriors(K):  \n",
    "    beta_0 = np.zeros(shape=(K, 2))\n",
    "    A_0 = get_diag(shape=(K, K), val=0.01) # diffuse precision matrix\n",
    "    # set diffuse hyper-parameters for 2nd-stage prior of gamma_0; follows defaults from rmultireg example\n",
    "    nu_00 = 3 + K  # 30\n",
    "    gamma_00 = get_diag(shape=(2, 2), val=nu_00) # diffuse precision matrix\n",
    "    hyper_prior = {'beta_0': beta_0, 'A_0':A_0, 'nu_00':nu_00, 'gamma_00':gamma_00}\n",
    "    return hyper_prior\n",
    "\n",
    "def draw_z(data, level_1, level_1_params_map):\n",
    "    tx = data['t_cal']\n",
    "    Tcal = data['T_cal']\n",
    "    p_lambda = level_1[level_1_params_map['lambda'], ]\n",
    "    p_mu = level_1[level_1_params_map['mu'], ]\n",
    "    mu_lam = p_mu + p_lambda\n",
    "    t_diff = Tcal - tx\n",
    "    prob = 1 / (1 + (p_mu / mu_lam) * (np.exp(mu_lam * t_diff) - 1))\n",
    "    z = (np.random.uniform(size=len(prob)) < prob)\n",
    "    z[z == True] = 1\n",
    "    z = z.astype(int)\n",
    "    return list(z.values)\n",
    "\n",
    "def draw_tau(data, level_1, level_1_params_map):\n",
    "    N = len(data)\n",
    "    tx = data['t_cal']\n",
    "    Tcal = data['T_cal']\n",
    "    p_lambda = level_1[level_1_params_map['lambda'], ]\n",
    "    p_mu = level_1[level_1_params_map['mu'], ]\n",
    "\n",
    "    mu_lam = p_mu + p_lambda\n",
    "    z = level_1[level_1_params_map['z'], ]\n",
    "\n",
    "    alive = (z == 1)\n",
    "    tau = np.zeros(shape=(N))\n",
    "\n",
    "    # Case: still alive - left truncated exponential distribution -> [T.cal, Inf]\n",
    "    if (np.sum(alive) > 0):\n",
    "        tau[alive] = Tcal[alive] + np.random.exponential(scale=1.0/p_mu[alive], size=np.sum(alive))\n",
    "\n",
    "    # Case: churned - double truncated exponential distribution -> [tx, T.cal]\n",
    "    if (np.sum(~alive) > 0):\n",
    "        mu_lam_tx = np.minimum(700, mu_lam[~alive] * tx[~alive])\n",
    "        mu_lam_Tcal = np.minimum(700, mu_lam[~alive] * Tcal[~alive])\n",
    "        rand = np.random.uniform(size=np.sum(~alive))        \n",
    "        tau[~alive] = (-1.0 * np.log((1.0 - rand) * np.exp(-1.0 * mu_lam_tx) + rand * np.exp((-1.0 * mu_lam_Tcal)))) / mu_lam[~alive]\n",
    "\n",
    "    return tau\n",
    "\n",
    "def chol2inv(chol):\n",
    "    return cho_solve((chol, False), np.eye(chol.shape[0])) \n",
    "\n",
    "def draw_wishart(df, scale):\n",
    "    W = wishart.rvs(df, scale)\n",
    "    IW = inv(W)\n",
    "    C = cholesky(W).T\n",
    "    CI = inv(C)\n",
    "    return W, IW, C, CI\n",
    "\n",
    "def rmultireg(Y, X, Bbar, A, nu, V):\n",
    "    # standard multi-variate normal regression update\n",
    "    # Slide 33 in http://ice.uchicago.edu/2008_presentations/Rossi/ICE_tutorial_2008.pdf\n",
    "    n = Y.shape[0]\n",
    "    m = Y.shape[1]\n",
    "    k = X.shape[1]    \n",
    "\n",
    "    RA = cholesky(A)\n",
    "    W = np.concatenate((X, RA), axis=0) \n",
    "    Z = np.concatenate((Y, RA*Bbar), axis=0)\n",
    "    IR = solve(np.triu(cholesky(np.dot(W.T, W)).T), np.eye(k,k)) #trimatu interprets the matrix as upper triangular and makes solve more efficient\n",
    "    Btilde = np.dot(np.dot(IR, IR.T), np.dot(W.T,Z))\n",
    "    E = Z - np.dot(W, Btilde)\n",
    "    S = np.dot(E.T, E)\n",
    "    W, IW, C, CI = draw_wishart(df=nu+n, scale=chol2inv(cholesky(V+S).T))\n",
    "    samples = np.random.normal(size=k*m).reshape(k,m)\n",
    "    B = Btilde + np.dot(IR, np.dot(samples, CI.T))\n",
    "    return {'beta': B.T, 'gamma':IW}\n",
    "\n",
    "def draw_level_2(covars, level_1, level_1_params_map, hyper_prior):\n",
    "    # standard multi-variate normal regression update\n",
    "    Y = np.log(level_1[[level_1_params_map['lambda'], level_1_params_map['mu']],].T)\n",
    "    X = covars\n",
    "    Bbar = hyper_prior['beta_0']\n",
    "    A = hyper_prior['A_0']\n",
    "    nu = hyper_prior['nu_00']\n",
    "    V = hyper_prior['gamma_00']\n",
    "    \n",
    "    return rmultireg(Y, X, Bbar, A, nu, V)\n",
    "\n",
    "def log_post(log_theta, mvmean, x, z, Tcal, tau, inv_gamma):\n",
    "    log_lambda = log_theta[0,:] \n",
    "    log_mu = log_theta[1,:]\n",
    "    diff_theta = np.subtract(log_theta, mvmean.T)\n",
    "    diff_lambda = diff_theta[0,:]\n",
    "    diff_mu = diff_theta[1,:]\n",
    "    likel = (x * log_lambda) + ((1 - z) * log_mu) - (((z * Tcal) + (1 - z) * tau) * (np.exp(log_lambda) + np.exp(log_mu)))\n",
    "    prior = -0.5 * ((np.square(diff_lambda) * inv_gamma[0, 0]) + (2 * np.multiply(diff_lambda, diff_mu) * inv_gamma[0, 1]) + (np.square(diff_mu) * inv_gamma[1, 1]))\n",
    "    post = np.add(likel[0], prior)    \n",
    "    post[0,log_mu > 5] = np.NINF  # cap !!\n",
    "    return post\n",
    "\n",
    "def step(cur_log_theta, cur_post, gamma, N, mvmean, x, z, Tcal, tau, inv_gamma):\n",
    "    new_log_theta = cur_log_theta + np.vstack((gamma[0, 0] * np.random.standard_t(df=3, size=N), gamma[1, 1] * np.random.standard_t(df=3, size=N)))\n",
    "    new_log_theta[0,:] = np.maximum(np.minimum(new_log_theta[0,:], 70), -70)\n",
    "    new_log_theta[1,:] = np.maximum(np.minimum(new_log_theta[1,:], 70), -70)\n",
    "    new_post = log_post(new_log_theta, mvmean, x, z, Tcal, tau, inv_gamma)\n",
    "    # accept/reject new proposal\n",
    "    mhratio = np.exp(new_post - cur_post)\n",
    "    unif = np.random.uniform(size=N)\n",
    "    accepted = np.asarray(mhratio > unif)[0]\n",
    "    cur_log_theta[:,accepted] = new_log_theta[:, accepted]\n",
    "    cur_post[0,accepted] = new_post[0,accepted]\n",
    "    return {'cur_log_theta':cur_log_theta, 'cur_post':cur_post}\n",
    "\n",
    "def draw_level_1(data, covars, level_1, level_1_params_map, level_2):\n",
    "    # sample (lambda, mu) given (z, tau, beta, gamma)\n",
    "    N = len(data)\n",
    "    x = data['x_cal']\n",
    "    Tcal = data['T_cal']\n",
    "    z = level_1[level_1_params_map['z'], ]\n",
    "    tau = level_1[level_1_params_map['tau'], ]\n",
    "    mvmean = np.dot(covars, level_2['beta'].T)\n",
    "    gamma = level_2['gamma']\n",
    "    inv_gamma = inv(gamma)\n",
    "    \n",
    "    cur_lambda = level_1[level_1_params_map['lambda'], ]\n",
    "    cur_mu = level_1[level_1_params_map['mu'], ]\n",
    "\n",
    "    # current state\n",
    "    cur_log_theta = np.vstack((np.log(cur_lambda), np.log(cur_mu)))\n",
    "    cur_post = log_post(cur_log_theta, mvmean, x, z, Tcal, tau, inv_gamma)\n",
    "    \n",
    "    iter = 1  # how high do we need to set this? 1/5/10/100?\n",
    "    for i in range(0, iter):\n",
    "        draw = step(cur_log_theta, cur_post, gamma, N, mvmean, x, z, Tcal, tau, inv_gamma)\n",
    "        cur_log_theta = draw['cur_log_theta']\n",
    "        cur_post = draw['cur_post']\n",
    "\n",
    "    cur_theta = np.exp(cur_log_theta)\n",
    "\n",
    "    return {'lambda':cur_theta[0,:], 'mu':cur_theta[1,:]}\n",
    "\n",
    "def run_single_chain(data, covariates, K, hyper_prior, nsample, nburnin, nskip):\n",
    "    ## initialize arrays for storing draws ##\n",
    "    LOG_LAMBDA = 0\n",
    "    LOG_MU = 1\n",
    "    nr_of_cust = len(data)\n",
    "    #nr_of_draws = nburnin + nsample * nskip\n",
    "    nr_of_draws = nburnin + nsample\n",
    "\n",
    "    # The 4 is for \"lambda\", \"mu\", \"tau\", \"z\"\n",
    "    level_1_params_map = get_map_from_array(['lambda', 'mu', 'tau', 'z'])\n",
    "    level_1_draws = np.zeros(shape=(nsample, 4, nr_of_cust))\n",
    "\n",
    "    level_2_draws = np.zeros(shape=(nsample, (2*K)+3))\n",
    "    nm = ['log_lambda', 'log_mu']\n",
    "    if (K > 1):\n",
    "        nm = ['{}_{}'.format(val2, val1) for val1 in covariates for val2 in nm]\n",
    "    nm.extend(['var_log_lambda', 'cov_log_lambda_log_mu', 'var_log_mu'])\n",
    "    level_2_params_map = get_map_from_array(nm)\n",
    "        \n",
    "    ## initialize parameters ##\n",
    "    data['t_cal_tmp'] = data['t_cal']\n",
    "    data['t_cal_tmp'][data.t_cal == 0] = data['T_cal'][data.t_cal == 0] \n",
    "    level_1 = level_1_draws[1,]\n",
    "    x_cal_mean = np.mean(data['x_cal'])\n",
    "    t_cal_tmp_mean = np.mean(data['t_cal_tmp'])\n",
    "    level_1[level_1_params_map['lambda'], ] = x_cal_mean/t_cal_tmp_mean\n",
    "    level_1[level_1_params_map['mu'], ] = 1 / (data['t_cal'] + 0.5 / level_1[level_1_params_map['lambda'], ])\n",
    "    \n",
    "    ## run MCMC chain ##\n",
    "    hyper_prior['beta_0'][0, LOG_LAMBDA] = np.log(np.mean(level_1[level_1_params_map['lambda'], ]))\n",
    "    hyper_prior['beta_0'][0, LOG_MU] = np.log(np.mean(level_1[level_1_params_map['mu'], ]))\n",
    "    \n",
    "    for i in range(0, nr_of_draws):\n",
    "        # draw individual-level parameters\n",
    "        level_1[level_1_params_map['z'], ] = draw_z(data, level_1, level_1_params_map)\n",
    "        level_1[level_1_params_map['tau'], ] = draw_tau(data, level_1, level_1_params_map)\n",
    "\n",
    "        level_2 = draw_level_2(covars, level_1, level_1_params_map, hyper_prior)\n",
    "        draw = draw_level_1(data, covars, level_1, level_1_params_map, level_2)\n",
    "        level_1[level_1_params_map['lambda'], ] = draw[\"lambda\"]\n",
    "        level_1[level_1_params_map['mu'], ] = draw[\"mu\"]\n",
    "        \n",
    "        #nk = int(round((i - nburnin) / nskip))        \n",
    "        if (i >= nburnin):\n",
    "            #Store\n",
    "            idx = i - nburnin\n",
    "            level_1_draws[idx,:,:] = level_1 # nolint\n",
    "            level_2_draws[idx,:] = list(np.array(level_2['beta'].T).reshape(-1)) + [level_2['gamma'][0, 0], level_2['gamma'][0, 1], level_2['gamma'][1,1]]\n",
    "        if (i % 100) == 0:\n",
    "            print('draw: {}'.format(i))\n",
    "    coeff_mean = np.mean(level_2_draws, axis=0)\n",
    "    coeff_stddev = np.std(level_2_draws, axis=0)    \n",
    "    coeff = {}\n",
    "    for param in level_2_params_map:\n",
    "        coeff[param] = {}\n",
    "        coeff[param]['mean'] = coeff_mean[level_2_params_map[param]]\n",
    "        coeff[param]['stddev'] = coeff_stddev[level_2_params_map[param]]\n",
    "    \n",
    "    return {\"level_1\":level_1_draws, \"level_1_params_map\":level_1_params_map\n",
    "          , \"level_2\":level_2_draws, \"level_2_params_map\":level_2_params_map\n",
    "          , \"coeff\": coeff}    \n",
    "\n",
    "####MCMC Functions\n",
    "def get_correlation(draws):\n",
    "    l2pmap = draws[\"level_2_params_map\"]\n",
    "    draw_means = np.mean(draws['level_2'], axis=0)\n",
    "    corr = draw_means[l2pmap['cov_log_lambda_log_mu']]/(np.sqrt(draw_means[l2pmap['var_log_lambda']]) * np.sqrt(draw_means[l2pmap['var_log_mu']]))\n",
    "    return corr\n",
    "\n",
    "def get_nr_of_cust(draws):\n",
    "    nr_of_cust = draws[\"level_1\"].shape[2]\n",
    "    return nr_of_cust\n",
    "\n",
    "def PAlive(draws):\n",
    "    l1pmap = draws[\"level_1_params_map\"]\n",
    "    nr_of_cust = get_nr_of_cust(draws)\n",
    "    p_alive = np.mean(draws[\"level_1\"][:,l1pmap['z'],:], axis=1)\n",
    "    return p_alive\n",
    "\n",
    "def draw_left_truncated_gamma(lower, k, lamda):\n",
    "    pg = gamma.cdf(x=lower, a=k, scale=1.0/(k*lamda))\n",
    "    rand = np.random.uniform(1, pg, 1)\n",
    "    qg = gamma.ppf(q=rand, a=k, scale=1.0/(k*lamda))\n",
    "    return qg\n",
    "\n",
    "def DrawFutureTransactions(data, draws, sample_size=None):\n",
    "    nr_of_draws = draws[\"level_2\"].shape[0]\n",
    "    if sample_size is not None:\n",
    "            nr_of_draws = sample_size\n",
    "    nr_of_cust = get_nr_of_cust(draws)\n",
    "    parameters = draws[\"level_1_params_map\"]\n",
    "    x_holdout = np.zeros(shape=(nr_of_draws, nr_of_cust))\n",
    "    t_cal = data['t_cal']\n",
    "    T_holdout = data['T_holdout']\n",
    "    T_cal = data['T_cal']\n",
    "\n",
    "    for i in range(0, nr_of_cust):\n",
    "        print('...processing customer: {} of {}'.format(i, nr_of_cust))\n",
    "        Tcal = T_cal[i]\n",
    "        Tholdout = T_holdout[i]\n",
    "        tcal = t_cal[i]\n",
    "        taus = draws['level_1'][:,parameters['tau'],i]\n",
    "        ks = np.ones(shape=(len(taus)))    \n",
    "        lamdas = draws['level_1'][:,parameters['lambda'],i]\n",
    "        if sample_size is not None:\n",
    "            taus = taus[sample_size]\n",
    "            ks = ks[sample_size]\n",
    "            lambdas = lambdas[sample_size]\n",
    "\n",
    "        alive = taus > Tcal\n",
    "        # Case: customer alive\n",
    "        idx = 0\n",
    "        for alive_val in alive:\n",
    "            if alive_val:\n",
    "                # sample itt which is larger than (Tcal-tx)\n",
    "                itts = draw_left_truncated_gamma(Tcal - tcal, ks[idx], lamdas[idx])\n",
    "                # sample 'sufficiently' large amount of inter-transaction times\n",
    "                minT = np.minimum(Tcal + Tholdout - tcal, taus[idx] - tcal)\n",
    "                nr_of_itt_draws = int(np.maximum(10, np.round(minT * lamdas[idx])))\n",
    "                itts = np.hstack((itts, np.array(gamma.rvs(a=ks[idx], loc=ks[idx]*lamdas[idx], size=nr_of_itt_draws*2))))\n",
    "                if (np.sum(itts) < minT):\n",
    "                    itts = np.hstack((itts, np.array(gamma.rvs(a=ks[idx], loc=ks[idx]*lamdas[idx], size=nr_of_itt_draws*4))))\n",
    "                if (np.sum(itts) < minT):\n",
    "                    itts = np.hstack((itts, np.array(gamma.rvs(a=ks[idx], loc=ks[idx]*lamdas[idx], size=nr_of_itt_draws*800))))\n",
    "                if (np.sum(itts) < minT):\n",
    "                    print(\"...not enough inter-transaction times sampled! cust: {}, draw: {}, {} < {}\".format(i, idx, np.sum(itts), minT))\n",
    "                x_holdout[idx, i] = np.sum(np.cumsum(itts) < minT)\n",
    "            idx += 1\n",
    "        if (np.any(~alive)):\n",
    "            x_holdout[~alive, i] = 0\n",
    "    return x_holdout\n",
    "\n",
    "def PActive(x_holdout_draws):\n",
    "    nr_of_cust = x_holdout_draws.shape[1]\n",
    "    p_alive = np.zeros(shape=(nr_of_cust))    \n",
    "    for i in range(0, nr_of_cust):\n",
    "        cd = x_holdout_draws[:,i]\n",
    "        p_alive[i] = np.mean(cd[cd > 0])\n",
    "    return p_alive\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main routine\n",
    "parse_dates = ['first']\n",
    "g_cbs_dataset = '{}/cbs.csv'.format(g_datafolder)\n",
    "df = load_dataset(g_cbs_dataset, parse_dates=parse_dates)\n",
    "covariates, covars, K = set_regressors(df, covariates=[\"first_sales\"])\n",
    "hyper_prior = set_hyperpriors(K)\n",
    "draws = run_single_chain(df, covariates=covariates, K=K, hyper_prior=hyper_prior, nsample=500, nburnin=500, nskip=10)\n",
    "#x_holdout_draws = DrawFutureTransactions(df, draws, sample_size=None)\n",
    "#df['x_predicted'] = np.mean(x_holdout_draws, axis=0)\n",
    "#p_alive = PActive(x_holdout_draws)\n",
    "#df['palive'] = p_alive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draws['coeff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_holdout_draws = DrawFutureTransactions(df, draws, sample_size=None)\n",
    "df['x_predicted'] = np.mean(x_holdout_draws, axis=0)\n",
    "p_alive = PActive(x_holdout_draws)\n",
    "df['palive'] = p_alive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(df['x_holdout'] - df['x_predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df.x_holdout.values, df.x_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['x_cal', 'x_holdout', 'x_predicted']].groupby(['x_cal']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
